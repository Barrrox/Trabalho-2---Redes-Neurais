# -*- coding: utf-8 -*-
"""Main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1anyVn8cjzlXelGme8SQEf-Axg2DwKUkw

# Classificação de obras de arte por movimento artístico

Autores: Ellen Brzozoski, João Silva, Lóra, Matheus Barros

# Imports
"""

import numpy as np
from random import randint
from tensorflow.keras.models import Model # type: ignore
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Input, Concatenate # type: ignore
from tensorflow.keras.utils import to_categorical # type: ignore
from scikeras.wrappers import KerasClassifier
from sklearn.model_selection import RandomizedSearchCV
from time import time

"""# Funções auxiliares

## FormatImage

A função formatImage recebe uma imagem desformatada como parametro e recorta ela para ter um tamanho fixo definido.
"""

"""Carregar base de dados"""


def otimizar_parametros(TAM_TESTES):
  
  inicio = time()

  # Carrega o conjunto de dados
  x_train = np.load("imagens_treino.npy")
  y_train = np.load("labels_treino.npy")

  print(f"Tempo = {time() - inicio:2f}s : Dados de treino carregados")


  # EScolhe aleatoriamente um conjunto da base de treino
  x_test = []
  y_test = []
  for _ in range(int(len(y_train)*TAM_TESTES)):
    imagem_index = randint(0, len(y_train) - 1)
    x_test.append(x_train[imagem_index])
    y_test.append(y_train[imagem_index])

  print(f"Tempo = {time() - inicio:2f}s : Dados de teste criados")

  # Trocando o conjunto de testes completo pelo conjunto parcial
  x_train = np.array(x_test)
  y_train = np.array(y_test)

  # Normaliza os valores dos pixels para que fiquem entre 0 e 1
  x_train = x_train / 255.0

  print(f"Tempo = {time() - inicio:2f}s : Pixels normalizados entre 0 e 1")

  # Converte os rótulos (labels) para categorias (one-hot encoding)
  y_train = to_categorical(y_train, 9)

  print(f"Tempo = {time() - inicio:2f}s : Rótulos covertidos para categorias")

  #2. Definindo o Modelo e Treinando

  def construir_modelo_inception(optimizer='adam', dense_units=128):
    # (Você precisaria colocar a função inception_module aqui dentro ou importá-la)
    def inception_module(x, f1, f2_in, f2_out, f3_in, f3_out, f4_out):
        # Ramo 1
        conv1 = Conv2D(f1, (1,1), padding='same', activation='relu')(x)
        # Ramo 2
        conv3_in = Conv2D(f2_in, (1,1), padding='same', activation='relu')(x)
        conv3 = Conv2D(f2_out, (3,3), padding='same', activation='relu')(conv3_in)
        # Ramo 3
        conv5_in = Conv2D(f3_in, (1,1), padding='same', activation='relu')(x)
        conv5 = Conv2D(f3_out, (5,5), padding='same', activation='relu')(conv5_in)
        # Ramo 4
        pool = MaxPooling2D((3,3), strides=(1,1), padding='same')(x)
        pool_proj = Conv2D(f4_out, (1,1), padding='same', activation='relu')(pool)
        # Concatenar
        output = Concatenate(axis=-1)([conv1, conv3, conv5, pool_proj])
        return output

    input_layer = Input(shape=(255, 255, 3))
    x = Conv2D(32, (3, 3), activation='relu', strides=(2, 2), padding='same')(input_layer)
    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same')(x)
    
    # Aqui você poderia parametrizar os filtros dos módulos Inception também
    x = inception_module(x, 64, 96, 128, 16, 32, 32)
    x = inception_module(x, 128, 128, 192, 32, 96, 64)
    
    x = MaxPooling2D((2, 2), strides=(2, 2), padding='same')(x)
    x = Flatten()(x)
    
    # Este é um dos parâmetros que você está otimizando
    x = Dense(dense_units, activation='relu')(x)
    output_layer = Dense(9, activation='softmax')(x)
    
    model = Model(inputs=input_layer, outputs=output_layer)
    
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
    
    return model
  
  # Empacotar
  model = KerasClassifier(model=construir_modelo_inception, verbose=2)

  print(f"Tempo = {time() - inicio:2f}s : Modelo empacotado com KerasClassifier")

  # Espaço de busca dos hiperparâmetros (versão recomendada)
  parametros = {
    # Parâmetros que vão para o .fit() do KerasClassifier
    'batch_size': [16, 32],
    'epochs': [20], # Aumente as épocas para dar tempo ao modelo de aprender

    # Parâmetros que vão para a função construir_modelo_inception (prefixo 'model__')
    'model__optimizer': ['adam', 'rmsprop'],
    'model__dense_units': [128, 256, 512],
  }


  # Randomized Search
  random_search = RandomizedSearchCV(estimator=model, param_distributions=parametros,
                                    n_iter=8, cv=2, verbose=2)
  

  print(f"Tempo = {time() - inicio:2f}s : Instancia do RandomizedSearch criada")

  # Executar o ajuste
  modelo_convergido = random_search.fit(x_train, y_train)

  print(f"Tempo = {time() - inicio:2f}s : Modelo tunado")

  print(f"Árvore de decisão dos parâmetros tunados: {modelo_convergido.best_params_}")
  print()
  print(f"O melhor score é {modelo_convergido.best_score_}")




def main():

  # Tamanho do conjunto de dados de testes em relação ao de treino
  # Ex: se TAM_TESTES = 0.16 então o conjunto de testes terá 16% do tamanho
  # do conjunto de treino
  TAM_TESTES = 0.15
  otimizar_parametros(TAM_TESTES)



if __name__ == "__main__":
  main()